{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hexagons\n",
    "trips_df_1h_r7 = vaex.open('data/hexagon_files/trips_grouped_1h_r7.hdf5')\n",
    "trips_df_1h_r8 = vaex.open('data/hexagon_files/trips_grouped_1h_r8.hdf5')\n",
    "trips_df_1h_r9 = vaex.open('data/hexagon_files/trips_grouped_1h_r9.hdf5')\n",
    "\n",
    "trips_df_4h_r7 = vaex.open('data/hexagon_files/trips_grouped_4h_r7.hdf5')\n",
    "trips_df_4h_r8 = vaex.open('data/hexagon_files/trips_grouped_4h_r8.hdf5')\n",
    "trips_df_4h_r9 = vaex.open('data/hexagon_files/trips_grouped_4h_r9.hdf5')\n",
    "\n",
    "trips_df_6h_r7 = vaex.open('data/hexagon_files/trips_grouped_6h_r7.hdf5')\n",
    "trips_df_6h_r8 = vaex.open('data/hexagon_files/trips_grouped_6h_r8.hdf5')\n",
    "trips_df_6h_r9 = vaex.open('data/hexagon_files/trips_grouped_6h_r9.hdf5')\n",
    "\n",
    "\n",
    "#Census Tract and Community Area data\n",
    "trips_df_1h_census = vaex.open('data/hexagon_files/trips_grouped_1h_census.hdf5')\n",
    "trips_df_1h_carea = vaex.open('data/hexagon_files/trips_grouped_1h_comArea.hdf5')\n",
    "\n",
    "trips_df_4h_census = vaex.open('data/hexagon_files/trips_grouped_4h_census.hdf5')\n",
    "trips_df_4h_carea = vaex.open('data/hexagon_files/trips_grouped_4h_comArea.hdf5')\n",
    "\n",
    "trips_df_6h_census = vaex.open('data/hexagon_files/trips_grouped_6h_census.hdf5')\n",
    "trips_df_6h_carea = vaex.open('data/hexagon_files/trips_grouped_6h_comArea.hdf5')\n",
    "\n",
    "df_tracts = gpd.read_file('data/raw/census_tract_geometry.geojson')\n",
    "df_commarea = gpd.read_file('data/raw/community_area_geometry.geojson')\n",
    "\n",
    "#Weather\n",
    "weather_df_1h = pd.read_pickle('data/weather_data_hourly_1.pickle')\n",
    "weather_df_4h = pd.read_pickle('data/weather_data_hourly_4.pickle')\n",
    "weather_df_6h = pd.read_pickle('data/weather_data_hourly_6.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the Weather data with the groupings of 1h\n",
    "trip_attributes_1h_r7 = ['ts_start_1_hour', 'start_month', 'start_weekday', 'start_1_hour', 'pickup_hex_7', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_1h_r7  = trips_df_1h_r7[trip_attributes_1h_r7].to_pandas_df()\n",
    "trips_df_1h_r7_w = trips_df_1h_r7.merge(weather_df_1h, left_on='ts_start_1_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_1h_r8 = ['ts_start_1_hour', 'start_month', 'start_weekday', 'start_1_hour', 'pickup_hex_8', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_1h_r8  = trips_df_1h_r8[trip_attributes_1h_r8].to_pandas_df()\n",
    "trips_df_1h_r8_w = trips_df_1h_r8.merge(weather_df_1h, left_on='ts_start_1_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_1h_r9 = ['ts_start_1_hour', 'start_month', 'start_weekday', 'start_1_hour', 'pickup_hex_9', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_1h_r9  = trips_df_1h_r9[trip_attributes_1h_r9].to_pandas_df()\n",
    "trips_df_1h_r9_w = trips_df_1h_r9.merge(weather_df_1h, left_on='ts_start_1_hour', right_on='datetime', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the Weather data with the groupings of 4h\n",
    "trip_attributes_4h_r7 = ['ts_start_4_hour', 'start_month', 'start_weekday', 'start_4_hour', 'pickup_hex_7', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_4h_r7  = trips_df_4h_r7[trip_attributes_4h_r7].to_pandas_df()\n",
    "trips_df_4h_r7_w = trips_df_4h_r7.merge(weather_df_4h, left_on='ts_start_4_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_4h_r8 = ['ts_start_4_hour', 'start_month', 'start_weekday', 'start_4_hour', 'pickup_hex_8', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_4h_r8  = trips_df_4h_r8[trip_attributes_4h_r8].to_pandas_df()\n",
    "trips_df_4h_r8_w = trips_df_4h_r8.merge(weather_df_4h, left_on='ts_start_4_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_4h_r9 = ['ts_start_4_hour', 'start_month', 'start_weekday', 'start_4_hour', 'pickup_hex_9', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_4h_r9  = trips_df_4h_r9[trip_attributes_4h_r9].to_pandas_df()\n",
    "trips_df_4h_r9_w = trips_df_4h_r9.merge(weather_df_4h, left_on='ts_start_4_hour', right_on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the Weather data with the groupings of 6h\n",
    "trip_attributes_6h_r7 = ['ts_start_6_hour', 'start_month', 'start_weekday', 'start_6_hour', 'pickup_hex_7', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_6h_r7  = trips_df_6h_r7[trip_attributes_6h_r7].to_pandas_df()\n",
    "trips_df_6h_r7_w = trips_df_6h_r7.merge(weather_df_6h, left_on='ts_start_6_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_6h_r8 = ['ts_start_6_hour', 'start_month', 'start_weekday', 'start_6_hour', 'pickup_hex_8', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_6h_r8  = trips_df_6h_r8[trip_attributes_6h_r8].to_pandas_df()\n",
    "trips_df_6h_r8_w = trips_df_6h_r8.merge(weather_df_6h, left_on='ts_start_6_hour', right_on='datetime', how='left')\n",
    "\n",
    "trip_attributes_6h_r9 = ['ts_start_6_hour', 'start_month', 'start_weekday', 'start_6_hour', 'pickup_hex_9', 'demand', 'Trip Seconds', 'Trip Miles', 'Fare', 'Trip Total']\n",
    "trips_df_6h_r9  = trips_df_6h_r9[trip_attributes_6h_r9].to_pandas_df()\n",
    "trips_df_6h_r9_w = trips_df_6h_r9.merge(weather_df_6h, left_on='ts_start_6_hour', right_on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to convert the hexagons to polygons using H3\n",
    "def convert_hex_to_polygon(hex):\n",
    "        polygon = None\n",
    "        if hex:\n",
    "               polygon = Polygon(h3.h3_to_geo_boundary(hex, geo_json=True))         \n",
    "        return polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to aggregate the trips for census tract and community area pickup and dropoff to create a geodataframe for visual representation\n",
    "def create_geodataframe(column_groupby, array_drop_columns, column_join, geometry, merge_df):\n",
    "\n",
    "    #Groupby spatial unit and count-aggregate for the trips started. Create a pandas-dataframe based on that\n",
    "    gdf = df_ct_ca.groupby(by= column_groupby, progress=True).agg({ 'ts_start':'count' }).to_pandas_df()\n",
    "\n",
    "    #Making sure no NaN or \"None\" values are present in the dataframe\n",
    "    gdf = gdf[(gdf[column_groupby].notna()) & (gdf[column_groupby] != \"None\")]\n",
    "\n",
    "    #Merge the taxi dataframe with the external dataframe containing the geometry information based on the given identifier column\n",
    "    gdf = pd.merge(gdf, merge_df, left_on = column_groupby, right_on = column_join)\n",
    "\n",
    "    #Drop non-essential columns\n",
    "    gdf.drop(array_drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "    gdf = gdf.drop_duplicates()\n",
    "\n",
    "    #Create the geodataframe and specify the geometry column for visual representation\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to aggregate the trips for census tract and community area pickup and dropoff to create a geodataframe for visual representation\n",
    "def create_geodataframe(column_groupby, array_drop_columns, column_join, geometry, merge_df):\n",
    "\n",
    "    #Groupby spatial unit and count-aggregate for the trips started. Create a pandas-dataframe based on that\n",
    "    gdf = df_ct_ca.groupby(by= column_groupby, progress=True).agg({ 'ts_start':'count' }).to_pandas_df()\n",
    "\n",
    "    #Making sure no NaN or \"None\" values are present in the dataframe\n",
    "    gdf = gdf[(gdf[column_groupby].notna()) & (gdf[column_groupby] != \"None\")]\n",
    "\n",
    "    #Merge the taxi dataframe with the external dataframe containing the geometry information based on the given identifier column\n",
    "    gdf = pd.merge(gdf, merge_df, left_on = column_groupby, right_on = column_join)\n",
    "\n",
    "    #Drop non-essential columns\n",
    "    gdf.drop(array_drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "    gdf = gdf.drop_duplicates()\n",
    "\n",
    "    #Create the geodataframe and specify the geometry column for visual representation\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the four different dataframes \n",
    "df_census_pickup = create_geodataframe('Pickup Census Tract', ['geoid10'],'geoid10', 'geometry', df_tracts)\n",
    "df_census_dropoff = create_geodataframe('Dropoff Census Tract', ['geoid10'], 'geoid10', 'geometry', df_tracts)\n",
    "df_carea_pickup = create_geodataframe('Pickup Community Area', ['commarea'], 'commarea', 'geometry', df_commarea)\n",
    "df_carea_dropoff = create_geodataframe('Dropoff Community Area', ['commarea'], 'commarea', 'geometry', df_commarea)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
