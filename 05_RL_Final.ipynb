{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh) and must be generated exactly when the driver wants to leave. The agentâ€™s goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ð›¼ð‘¡ is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "#Basic\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "# Gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "# Keras RL\n",
    "from rl.agents import DQNAgent\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Class\n",
    "The \"randomInit\" parameter gives us the option to initialize 2 different environments; One of them a little bit more advanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(Env):\n",
    "    def __init__(self, printB, randomInit):\n",
    "        #Possible Actions for charging zero, low, medium to high\n",
    "        self.action_space = Discrete(4)\n",
    "        #Vehicle's battery: 69KWh; Timeframe 2p.m. to 4p.m.: 8x 15 minute intervals\n",
    "        self.battery_limit = 69\n",
    "        self.observation_space = np.array([Box(low=np.array([0]), high = np.array([self.battery_limit])), Box(low=np.array([0]), high = np.array([8]))])\n",
    "        # Starting at 2p.m.: 0 (going to 3:45p.m.: 7)\n",
    "        self.time = 0\n",
    "        self.time_delta =  15/60\n",
    "        # Battery load at initialization\n",
    "        self.battery = 25\n",
    "        self.randomInit = randomInit\n",
    "        if self.randomInit:\n",
    "            self.battery += random.randint(-15,5)\n",
    "        # our state consisting of battery status and time\n",
    "        self.state = np.array([self.battery, self.time])\n",
    "        self.printB = printB\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Setting loading interval +1/8 /--> +15/120 minutes\n",
    "        self.time += 1\n",
    "        self.state[1] = self.time\n",
    "\n",
    "        # Seting new battery state\n",
    "        #zero\n",
    "        load = 0\n",
    "        if action == 2:\n",
    "            #low\n",
    "            load += 7 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        if action == 3:\n",
    "            #medium\n",
    "            load += 14 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        if action == 4:\n",
    "            #high\n",
    "            load += 22 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        # load multiplied by 15 min to get the \n",
    "        self.battery += load\n",
    "        self.state[0] = self.battery\n",
    "\n",
    "        # Cost function\n",
    "        reward = self.time * math.exp(load) * (-1)\n",
    "        # Because e^0 = 1\n",
    "        if action == 0:\n",
    "            reward = 0\n",
    "\n",
    "        if self.printB:\n",
    "            print(\"time:\" + str(self.time) + \" |load:\" + str(load) + \" |reward:\" + str(round(reward, 2)))\n",
    "\n",
    "        #Checking if 2 Hours are done\n",
    "        if self.time >= 8:\n",
    "            #Demand is a random value following a normal distribution (e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh)\n",
    "            kwh_needed = np.random.normal(loc=30, scale=5)\n",
    "            if self.printB:\n",
    "                print(\"|needed:\" + str(round(kwh_needed, 2))+\" |battery:\" + str(round(self.battery, 2)))\n",
    "            # The agentâ€™s goal is to avoid running out of energy (with a very high penalty) \n",
    "            if kwh_needed > self.battery:\n",
    "                #High Penalty\n",
    "                reward -= 10000\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # Starting at 2p.m.: 0 (going to 3:45p.m.: 7)\n",
    "        self.time = 0\n",
    "        # Battery load at initialization\n",
    "        self.battery = 25\n",
    "        if self.randomInit:\n",
    "            self.battery +=random.randint(-15,5)\n",
    "            if self.printB:\n",
    "                print(\"Initialized with :\" + str(self.battery) + \" KWh\")\n",
    "        # our state consisting of battery status and time\n",
    "        self.state = np.array([self.battery, self.time])\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Environment\n",
    "It has all the components that are required by the exercise.\n",
    "As the initialization is not specified in the exercise we initialize the car battery with 25KWh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "printB = False\n",
    "randomInit = False\n",
    "env = Environment(printB, randomInit)\n",
    "printB = True\n",
    "printEnv = Environment(printB, randomInit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4 | States: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Destilling important Information for our model\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(\"Actions: \" + str(actions) + \" | States: \" + str(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our model\n",
    "model = Sequential()    \n",
    "model.add(Flatten(input_shape=(1,2)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 8s 821us/step - reward: -148.9739\n",
      "1250 episodes - episode_reward: -1191.791 [-1192.156, -1110.074]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -156.2504\n",
      "1250 episodes - episode_reward: -1250.003 [-10403.385, 0.000] - loss: 298046.923 - mae: 143.749 - mean_q: -131.207\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -66.8957\n",
      "1250 episodes - episode_reward: -535.166 [-10207.166, -207.166] - loss: 384229.906 - mae: 361.060 - mean_q: -374.914\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -127.3336\n",
      "1250 episodes - episode_reward: -1018.669 [-10711.934, -13.000] - loss: 246899.781 - mae: 438.659 - mean_q: -479.478\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -277.7358\n",
      "1250 episodes - episode_reward: -2221.886 [-10749.216, -16.000] - loss: 580289.438 - mae: 542.407 - mean_q: -563.796\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -280.1383\n",
      "1250 episodes - episode_reward: -2241.106 [-10674.476, -13.000] - loss: 961144.812 - mae: 712.879 - mean_q: -782.422\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -282.8520\n",
      "1250 episodes - episode_reward: -2262.816 [-10729.952, -18.000] - loss: 973528.562 - mae: 887.678 - mean_q: -1029.423\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -272.4728\n",
      "1250 episodes - episode_reward: -2179.782 [-10673.721, -25.000] - loss: 991579.625 - mae: 1042.733 - mean_q: -1228.837\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -272.7519\n",
      "1250 episodes - episode_reward: -2182.015 [-10636.606, -29.509] - loss: 978651.938 - mae: 1157.262 - mean_q: -1362.899\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -281.8130\n",
      "done, took 688.608 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a09469960>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining and training of Deep Q-Network Agent\n",
    "policy = BoltzmannQPolicy()\n",
    "memory = SequentialMemory(limit=10000*2, window_length=1)\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10000*1, target_model_update=1e-2)\n",
    "dqn.compile(Adam(learning_rate=0.1), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=10000*10, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.23 |battery:39.0\n",
      "Episode 1: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:27.73 |battery:39.0\n",
      "Episode 2: reward: -207.166, steps: 8\n",
      "-207.1656963362063\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent on the simple Environment\n",
    "results = dqn.test(printEnv, nb_episodes=2, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the learned policy.\n",
    "\n",
    "The agent always loads 14 KWh in the two hours. The resulting battery state thus is 39KWh.\n",
    "He does it in the most efficient way, which is by loading with 7KW at all times.\n",
    "\n",
    "As the initialized battery state is always the same it does the same for all test episodes.\n",
    "Of course this result is dependent on the choosen very high penalty of -10000 if the car runs out of energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Environment\n",
    "With random battery initialization between 15 and 30 to have a better look at how the agent takes the battery state into account when acting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "printB = False\n",
    "randomInit = True\n",
    "env2 = Environment(printB, randomInit)\n",
    "printB = True\n",
    "printEnv2 = Environment(printB, randomInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -962.2452\n",
      "1250 episodes - episode_reward: -7697.962 [-10517.732, -33.755]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -649.9761\n",
      "1250 episodes - episode_reward: -5199.809 [-10967.348, 0.000] - loss: 1060532.947 - mae: 2848.122 - mean_q: -3171.436\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -487.0741\n",
      "1250 episodes - episode_reward: -3896.592 [-10860.659, -150.110] - loss: 627184.375 - mae: 2171.474 - mean_q: -2262.274\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: -468.1859\n",
      "1250 episodes - episode_reward: -3745.487 [-10845.150, -113.759] - loss: 377627.250 - mae: 1738.747 - mean_q: -1797.645\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -535.5441\n",
      "done, took 452.884 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a2101e590>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Destilling important Information for our model\n",
    "states = env2.observation_space.shape\n",
    "actions = env2.action_space.n\n",
    "model2 = Sequential()    \n",
    "model2.add(Flatten(input_shape=(1,2)))\n",
    "#model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "#model2.add(Dense(4, activation='relu'))\n",
    "model2.add(Dense(actions, activation='linear'))\n",
    "\n",
    "# Defining and training of Deep Q-Network Agent\n",
    "policy = BoltzmannQPolicy()\n",
    "memory = SequentialMemory(limit=10000*2, window_length=1)\n",
    "dqn2 = DQNAgent(model=model2, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10000*1, target_model_update=1e-2)\n",
    "dqn2.compile(Adam(learning_rate=0.005), metrics=['mae'])\n",
    "dqn2.fit(env2, nb_steps=10000*5, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the policy this agent learned and compare it to the policy of the simpler environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that the agent reacts to the battery state. If the initial state is lower he loads more compared to a higher initial battery state.\n",
    "He seems to try to get to a battery state of around 38 to 47 at the end of the two hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Initialized with :12 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:27.97 |battery:40.0\n",
      "Episode 1: reward: -1192.156, steps: 8\n",
      "-1192.1562705129234\n"
     ]
    }
   ],
   "source": [
    "results = dqn2.test(printEnv2, nb_episodes=1, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example, where the initialized 12KWh battery state makes the agent load with 14kw at all 15 minute intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Initialized with :30 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:24.37 |battery:47.5\n",
      "Episode 1: reward: -289.248, steps: 8\n",
      "-289.24824418426607\n"
     ]
    }
   ],
   "source": [
    "results = dqn2.test(printEnv2, nb_episodes=1, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example, where a high initialized battery makes the agent not load as much as in the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Initialized with :28 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:38.12 |battery:47.25\n",
      "Episode 1: reward: -371.331, steps: 8\n",
      "-371.3307920323258\n"
     ]
    }
   ],
   "source": [
    "results = dqn2.test(printEnv2, nb_episodes=1, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this example above is somewhere in between the examples before.\n",
    "\n",
    "You can also have a look at 25 random exmaples following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 25 episodes ...\n",
      "Initialized with :26 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:23.58 |battery:47.0\n",
      "Episode 1: reward: -480.774, steps: 8\n",
      "Initialized with :13 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:31.95 |battery:41.0\n",
      "Episode 2: reward: -1192.156, steps: 8\n",
      "Initialized with :13 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:30.26 |battery:41.0\n",
      "Episode 3: reward: -1192.156, steps: 8\n",
      "Initialized with :25 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:27.39 |battery:46.0\n",
      "Episode 4: reward: -480.774, steps: 8\n",
      "Initialized with :22 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:20.47 |battery:44.75\n",
      "Episode 5: reward: -617.578, steps: 8\n",
      "Initialized with :12 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:22.97 |battery:40.0\n",
      "Episode 6: reward: -1192.156, steps: 8\n",
      "Initialized with :20 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:29.84 |battery:44.5\n",
      "Episode 7: reward: -781.744, steps: 8\n",
      "Initialized with :30 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:27.0 |battery:47.5\n",
      "Episode 8: reward: -289.248, steps: 8\n",
      "Initialized with :13 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:41.4 |battery:41.0\n",
      "Episode 9: reward: -11192.156, steps: 8\n",
      "Initialized with :29 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:30.05 |battery:48.25\n",
      "Episode 10: reward: -398.692, steps: 8\n",
      "Initialized with :19 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:31.07 |battery:43.5\n",
      "Episode 11: reward: -781.744, steps: 8\n",
      "Initialized with :21 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:36.18 |battery:45.5\n",
      "Episode 12: reward: -781.744, steps: 8\n",
      "Initialized with :29 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:24.4 |battery:48.25\n",
      "Episode 13: reward: -398.692, steps: 8\n",
      "Initialized with :10 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:32.67 |battery:38.0\n",
      "Episode 14: reward: -1192.156, steps: 8\n",
      "Initialized with :11 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:30.57 |battery:39.0\n",
      "Episode 15: reward: -1192.156, steps: 8\n",
      "Initialized with :23 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:34.69 |battery:45.75\n",
      "Episode 16: reward: -617.578, steps: 8\n",
      "Initialized with :27 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:22.48 |battery:48.0\n",
      "Episode 17: reward: -480.774, steps: 8\n",
      "Initialized with :25 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.01 |battery:46.0\n",
      "Episode 18: reward: -480.774, steps: 8\n",
      "Initialized with :14 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:26.55 |battery:42.0\n",
      "Episode 19: reward: -1192.156, steps: 8\n",
      "Initialized with :20 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:25.74 |battery:44.5\n",
      "Episode 20: reward: -781.744, steps: 8\n",
      "Initialized with :20 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:24.76 |battery:44.5\n",
      "Episode 21: reward: -781.744, steps: 8\n",
      "Initialized with :28 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:36.36 |battery:47.25\n",
      "Episode 22: reward: -371.331, steps: 8\n",
      "Initialized with :12 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:28.99 |battery:40.0\n",
      "Episode 23: reward: -1192.156, steps: 8\n",
      "Initialized with :26 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:33.04 |battery:47.0\n",
      "Episode 24: reward: -480.774, steps: 8\n",
      "Initialized with :10 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:3.5 |reward:-165.58\n",
      "time:6 |load:3.5 |reward:-198.69\n",
      "time:7 |load:3.5 |reward:-231.81\n",
      "time:8 |load:3.5 |reward:-264.92\n",
      "|needed:24.47 |battery:38.0\n",
      "Episode 25: reward: -1192.156, steps: 8\n",
      "-1189.4045690717767\n"
     ]
    }
   ],
   "source": [
    "results = dqn2.test(printEnv2, nb_episodes=25, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
