{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh) and must be generated exactly when the driver wants to leave. The agentâ€™s goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ð›¼ð‘¡ is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "#Basic\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "# Gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "# Keras RL\n",
    "from rl.agents import DQNAgent\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating of Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(Env):\n",
    "    def __init__(self, printB, randomInit):\n",
    "        #Possible Actions for charging zero, low, medium to high\n",
    "        self.action_space = Discrete(4)\n",
    "        #Vehicle's battery: 69KWh; Timeframe 2p.m. to 4p.m.: 8x 15 minute intervals\n",
    "        self.battery_limit = 69\n",
    "        self.observation_space = np.array([Box(low=np.array([0]), high = np.array([self.battery_limit])), Box(low=np.array([0]), high = np.array([8]))])\n",
    "        # Starting at 2p.m.: 0 (going to 3:45p.m.: 7)\n",
    "        self.time = 0\n",
    "        self.time_delta =  15/60\n",
    "        # Battery load at initialization\n",
    "        self.battery = 25\n",
    "        self.randomInit = randomInit\n",
    "        if self.randomInit:\n",
    "            self.battery = 25 + random.randint(-5,5)\n",
    "        # our state consisting of battery status and time\n",
    "        self.state = np.array([self.battery, self.time])\n",
    "        self.printB = printB\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Setting loading interval +1/8 /--> +15/120 minutes\n",
    "        self.time += 1\n",
    "        self.state[1] = self.time\n",
    "\n",
    "        # Seting new battery state\n",
    "        #zero\n",
    "        load = 0\n",
    "        if action == 2:\n",
    "            #low\n",
    "            load += 7 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        if action == 3:\n",
    "            #medium\n",
    "            load += 14 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        if action == 4:\n",
    "            #high\n",
    "            load += 22 * self.time_delta\n",
    "            # loading until max capacity \n",
    "            if self.battery + load > self.battery_limit:\n",
    "                diff = self.battery_limit - self.battery\n",
    "                load = diff / self.time_delta\n",
    "        # load multiplied by 15 min to get the \n",
    "        self.battery += load\n",
    "        self.state[0] = self.battery\n",
    "\n",
    "        # Cost function\n",
    "        reward = self.time * math.exp(load) * (-1)\n",
    "        # Because e^0 = 1\n",
    "        if action == 0:\n",
    "            reward = 0\n",
    "\n",
    "        if self.printB:\n",
    "            print(\"time:\" + str(self.time) + \" |load:\" + str(load) + \" |reward:\" + str(round(reward, 2)))\n",
    "\n",
    "        #Checking if 2 Hours are done\n",
    "        if self.time >= 8:\n",
    "            #Demand is a random value following a normal distribution (e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh)\n",
    "            kwh_needed = np.random.normal(loc=30, scale=5)\n",
    "            if self.printB:\n",
    "                print(\"|needed:\" + str(round(kwh_needed, 2))+\" |battery:\" + str(round(self.battery, 2)))\n",
    "            # The agentâ€™s goal is to avoid running out of energy (with a very high penalty) \n",
    "            if kwh_needed > self.battery:\n",
    "                #print(\"NO BATTERY\")\n",
    "                reward -= 10000\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # Starting at 2p.m.: 0 (going to 3:45p.m.: 7)\n",
    "        self.time = 0\n",
    "        # Battery load at initialization\n",
    "        self.battery = 25\n",
    "        if self.randomInit:\n",
    "            self.battery = 25 + random.randint(-5,5)\n",
    "            if self.printB:\n",
    "                print(\"Initialized with :\" + str(self.battery) + \" KWh\")\n",
    "        # our state consisting of battery status and time\n",
    "        self.state = np.array([self.battery, self.time])\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "printB = False\n",
    "randomInit = False\n",
    "env = Environment(printB, randomInit)\n",
    "printB = True\n",
    "printEnv = Environment(printB, randomInit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4 | States: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Destilling important Information for our model\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(\"Actions: \" + str(actions) + \" | States: \" + str(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 2)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our model\n",
    "model = Sequential()    \n",
    "model.add(Flatten(input_shape=(1,2)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 40000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  127/10000 [..............................] - ETA: 7s - reward: -329.3679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 781us/step - reward: -464.2398\n",
      "1250 episodes - episode_reward: -3713.918 [-10192.902, -36.755]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -354.7510\n",
      "1250 episodes - episode_reward: -2838.008 [-10827.886, 0.000] - loss: 1985692.603 - mae: 213.255 - mean_q: -121.788\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -166.5225\n",
      "1250 episodes - episode_reward: -1332.180 [-10695.424, 0.000] - loss: 1321667.500 - mae: 384.488 - mean_q: -367.431\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -185.1807\n",
      "done, took 219.675 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2154243fa60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining and training of Deep Q-Network Agent\n",
    "policy = BoltzmannQPolicy()\n",
    "memory = SequentialMemory(limit=10000*2, window_length=1)\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10000*1, target_model_update=1e-2)\n",
    "dqn.compile(Adam(learning_rate=0.05), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=10000*4, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:20.2 |battery:39.0\n",
      "Episode 1: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:28.75 |battery:39.0\n",
      "Episode 2: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:36.61 |battery:39.0\n",
      "Episode 3: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.61 |battery:39.0\n",
      "Episode 4: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.74 |battery:39.0\n",
      "Episode 5: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:31.69 |battery:39.0\n",
      "Episode 6: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:37.31 |battery:39.0\n",
      "Episode 7: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:33.92 |battery:39.0\n",
      "Episode 8: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:29.2 |battery:39.0\n",
      "Episode 9: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.17 |battery:39.0\n",
      "Episode 10: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:28.21 |battery:39.0\n",
      "Episode 11: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:35.52 |battery:39.0\n",
      "Episode 12: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:29.07 |battery:39.0\n",
      "Episode 13: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:29.43 |battery:39.0\n",
      "Episode 14: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:39.8 |battery:39.0\n",
      "Episode 15: reward: -10207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:35.67 |battery:39.0\n",
      "Episode 16: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:34.3 |battery:39.0\n",
      "Episode 17: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:26.14 |battery:39.0\n",
      "Episode 18: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.36 |battery:39.0\n",
      "Episode 19: reward: -207.166, steps: 8\n",
      "time:1 |load:1.75 |reward:-5.75\n",
      "time:2 |load:1.75 |reward:-11.51\n",
      "time:3 |load:1.75 |reward:-17.26\n",
      "time:4 |load:1.75 |reward:-23.02\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:32.4 |battery:39.0\n",
      "Episode 20: reward: -207.166, steps: 8\n",
      "-707.1656963362062\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent on the simple Environment\n",
    "results = dqn.test(printEnv, nb_episodes=20, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Environment\n",
    "With random Battery initialization between 20 and 30 to have a better look at how the agent takes the current battery state into account when acting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "printB = False\n",
    "randomInit = True\n",
    "env2 = Environment(printB, randomInit)\n",
    "printB = True\n",
    "printEnv2 = Environment(printB, randomInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 9s 860us/step - reward: -1017.3355\n",
      "1250 episodes - episode_reward: -8138.684 [-10263.581, 0.000]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -164.5201\n",
      "1250 episodes - episode_reward: -1316.161 [-10875.511, -36.000] - loss: 983880.477 - mae: 1086.251 - mean_q: -1011.721\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -166.4822\n",
      "1250 episodes - episode_reward: -1331.858 [-10618.833, -20.509] - loss: 486143.375 - mae: 789.708 - mean_q: -775.298\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -180.2413\n",
      "1250 episodes - episode_reward: -1441.930 [-10704.179, -57.037] - loss: 348399.625 - mae: 497.468 - mean_q: -444.481\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -227.3222\n",
      "1250 episodes - episode_reward: -1818.577 [-10725.198, -39.773] - loss: 350977.625 - mae: 607.743 - mean_q: -566.582\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -239.8371\n",
      "1250 episodes - episode_reward: -1918.697 [-10738.952, -43.282] - loss: 369449.312 - mae: 742.853 - mean_q: -725.647\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -223.6891\n",
      "1250 episodes - episode_reward: -1789.513 [-10698.249, -33.264] - loss: 424723.562 - mae: 788.638 - mean_q: -791.546\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -238.5836\n",
      "1250 episodes - episode_reward: -1908.669 [-10865.002, -26.264] - loss: 483182.594 - mae: 718.965 - mean_q: -688.845\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -261.2022\n",
      "1250 episodes - episode_reward: -2089.617 [-10698.424, -9.000] - loss: 507288.531 - mae: 870.854 - mean_q: -868.445\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -232.9776\n",
      "done, took 699.681 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215466f89d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Destilling important Information for our model\n",
    "states = env2.observation_space.shape\n",
    "actions = env2.action_space.n\n",
    "model = Sequential()    \n",
    "model.add(Flatten(input_shape=(1,2)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "\n",
    "# Defining and training of Deep Q-Network Agent\n",
    "policy = BoltzmannQPolicy()\n",
    "memory = SequentialMemory(limit=10000*2, window_length=1)\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10000*1, target_model_update=1e-2)\n",
    "dqn.compile(Adam(learning_rate=0.01), metrics=['mae'])\n",
    "dqn.fit(env2, nb_steps=10000*10, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Initialized with :28 KWh\n",
      "time:1 |load:3.5 |reward:-33.12\n",
      "time:2 |load:3.5 |reward:-66.23\n",
      "time:3 |load:3.5 |reward:-99.35\n",
      "time:4 |load:3.5 |reward:-132.46\n",
      "time:5 |load:1.75 |reward:-28.77\n",
      "time:6 |load:1.75 |reward:-34.53\n",
      "time:7 |load:1.75 |reward:-40.28\n",
      "time:8 |load:1.75 |reward:-46.04\n",
      "|needed:30.64 |battery:49.0\n",
      "Episode 1: reward: -480.774, steps: 8\n",
      "-480.77418916307215\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent on the simple Environment\n",
    "results = dqn.test(printEnv2, nb_episodes=1, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
