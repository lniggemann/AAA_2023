{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to do some Reinforcement Learning -.-"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., ùúá= 30 kWh, ùúé = 5 kWh) and must be generated exactly when the driver wants to leave. The agent‚Äôs goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ùõºùë° is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high.\n",
    "\n",
    "**Bulletoints:**\n",
    "- Problem description:\n",
    "    - An electric taxi driver can charge her vehicle at home between 2 p.m. and 4 p.m. each day\n",
    "    - The charging agent adjusts the charging power every 15 minutes within a range of 0 kW to 22 kW\n",
    "    - The vehicle's battery has a limited capacity that cannot be exceeded\n",
    "    - The taxi needs enough energy to complete its working day, which is a random value following a normal distribution (e.g., ùúá= 30 kWh, ùúé = 5 kWh)\n",
    "    - The agent‚Äôs goal is to avoid running out of energy (with a very high penalty) and to minimize the recharging cost, which is an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ùõºùë° is the time coefficient and p is the charging rate\n",
    "- Task description:\n",
    "    - Create the environment that simulates the charging process and the energy demand, and returns the reward to the agent based on its actions\n",
    "    - Define a Markov decision process, including states, actions, and reward function, that models the problem\n",
    "    - Solve the Markov decision process using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies\n",
    "    - Consider only discrete action methods, such as zero, low, medium, high, for the charging power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
