{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to do some Reinforcement Learning -.-"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., ùúá= 30 kWh, ùúé = 5 kWh) and must be generated exactly when the driver wants to leave. The agent‚Äôs goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ùõºùë° is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high.\n",
    "\n",
    "\n",
    "**In Bulletpoints:**\n",
    "- Problem description:\n",
    "    - An electric taxi driver can charge her vehicle at home between 2 p.m. and 4 p.m. each day\n",
    "    - The charging agent adjusts the charging power every 15 minutes within a range of 0 kW to 22 kW\n",
    "    - The vehicle's battery has a limited capacity that cannot be exceeded\n",
    "    - The taxi needs enough energy to complete its working day, which is a random value following a normal distribution (e.g., ùúá= 30 kWh, ùúé = 5 kWh)\n",
    "    - The agent‚Äôs goal is to avoid running out of energy (with a very high penalty) and to minimize the recharging cost, which is an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ùõºùë° is the time coefficient and p is the charging rate\n",
    "- Task description:\n",
    "    - Create the environment that simulates the charging process and the energy demand, and returns the reward to the agent based on its actions\n",
    "    - Define a Markov decision process, including states, actions, and reward function, that models the problem\n",
    "    - Solve the Markov decision process using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies\n",
    "    - Consider only discrete action methods, such as zero, low, medium, high, for the charging power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try mit Hilfe von diesem Tutorial:\n",
    "# https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # a range of 0 kW to 22 kW\n",
    "        #self.action_space = Box(low=0, high=22)\n",
    "        #a range from zero, low, medium to high\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # The vehicle's battery has a limited capacity that cannot be exceeded (69KWh)\n",
    "        #self.battery_space = Box(low=0, high=69)\n",
    "        self.battery_space = Box(low=np.array([0]), high = np.array([69]))\n",
    "\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #print(\"-- New Step --\")\n",
    "        # Setting loading interval -1 /--> -15 minutes\n",
    "        self.time += 1\n",
    "\n",
    "\n",
    "        # Seting new battery state\n",
    "        #zero\n",
    "        load = 0\n",
    "        if action == 2:\n",
    "            #low\n",
    "            load += 7\n",
    "        if action == 3:\n",
    "            #medium\n",
    "            load += 14\n",
    "        if action == 4:\n",
    "            #high\n",
    "            load += 22\n",
    "        self.battery_state += load\n",
    "\n",
    "\n",
    "        # Calculating Negative Reward from Energy Costs\n",
    "        #Erstmal testweise andere funktion als von der aufgabe gegeben\n",
    "        kw_price = 1 #random.randint(0,3)\n",
    "        reward = kw_price * load * (-1)\n",
    "\n",
    "\n",
    "        #Checking if 2 Hours are done\n",
    "        #Giving panalty if car ran out of battery\n",
    "        if self.time >= 7:\n",
    "            #The taxi needs enough energy to complete its working day,\n",
    "            # which is a random value following a normal distribution\n",
    "            # (e.g., ùúá= 30 kWh, ùúé = 5 kWh)\n",
    "            kwh_needed = np.random.normal(loc=30, scale=5)\n",
    "            print(\"kwh needed:\" + str(kwh_needed))\n",
    "            print(\"battery state:\" + str(self.battery_state))\n",
    "            # The agent‚Äôs goal is to avoid running out of energy (with a very high penalty) \n",
    "            if kwh_needed > self.battery_state:\n",
    "                reward -= 100\n",
    "            #print(\"Day done!\")\n",
    "            day_done = True\n",
    "        else:\n",
    "            day_done = False\n",
    "\n",
    "        #print(\"Battery State: \" + str(self.battery_state))\n",
    "        #print(\"Reward: \" + str(reward))\n",
    "        # Returning the step information\n",
    "        return self.battery_state, reward, day_done\n",
    "    \n",
    "    #def render(self):\n",
    "        # blabala\n",
    "    \n",
    "    def reset(self):\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "        #return self.battery_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ Day 1 ___\n",
      "kwh needed:37.2269853666035\n",
      "battery state:86\n",
      "Episode:1 Score:-49\n",
      "__ Day 2 ___\n",
      "kwh needed:25.222470157449482\n",
      "battery state:45\n",
      "Episode:2 Score:-14\n",
      "__ Day 3 ___\n",
      "kwh needed:38.6562603492953\n",
      "battery state:60\n",
      "Episode:3 Score:-28\n",
      "__ Day 4 ___\n",
      "kwh needed:28.092133349815906\n",
      "battery state:76\n",
      "Episode:4 Score:-49\n",
      "__ Day 5 ___\n",
      "kwh needed:34.023391480321365\n",
      "battery state:42\n",
      "Episode:5 Score:-21\n",
      "__ Day 6 ___\n",
      "kwh needed:28.626844784852413\n",
      "battery state:59\n",
      "Episode:6 Score:-21\n",
      "__ Day 7 ___\n",
      "kwh needed:29.82157006750622\n",
      "battery state:47\n",
      "Episode:7 Score:-14\n"
     ]
    }
   ],
   "source": [
    "episodes = 7 #7 days\n",
    "for episode in range(1, episodes+1):\n",
    "    print(\"__ Day \" + str(episode) + \" ___\")\n",
    "    #battery_state = \n",
    "    env.reset()\n",
    "    day_done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not day_done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, day_done = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = env.battery_space.shape\n",
    "#states = int(69)\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(69, activation='relu', input_shape=(69,)))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 69)                4830      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 36)                2520      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 148       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,498\n",
      "Trainable params: 7,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=5000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(model, actions)\n\u001b[0;32m      2\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m dqn\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m60000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\rl\\core.py:134\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_observation(observation)\n\u001b[1;32m--> 134\u001b[0m \u001b[39massert\u001b[39;00m observation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# Perform random starts at beginning of episode and do not record them into the experience.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m# This slightly changes the start position between games.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m nb_random_start_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m nb_max_start_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(nb_max_start_steps)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
