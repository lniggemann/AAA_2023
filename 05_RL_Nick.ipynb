{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to do some Reinforcement Learning -.-"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh) and must be generated exactly when the driver wants to leave. The agentâ€™s goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ð›¼ð‘¡ is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high.\n",
    "\n",
    "\n",
    "**In Bulletpoints:**\n",
    "- Problem description:\n",
    "    - An electric taxi driver can charge her vehicle at home between 2 p.m. and 4 p.m. each day\n",
    "    - The charging agent adjusts the charging power every 15 minutes within a range of 0 kW to 22 kW\n",
    "    - The vehicle's battery has a limited capacity that cannot be exceeded\n",
    "    - The taxi needs enough energy to complete its working day, which is a random value following a normal distribution (e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh)\n",
    "    - The agentâ€™s goal is to avoid running out of energy (with a very high penalty) and to minimize the recharging cost, which is an exponential function of the power (i.e., ![image.png](attachment:image.png)), where ð›¼ð‘¡ is the time coefficient and p is the charging rate\n",
    "- Task description:\n",
    "    - Create the environment that simulates the charging process and the energy demand, and returns the reward to the agent based on its actions\n",
    "    - Define a Markov decision process, including states, actions, and reward function, that models the problem\n",
    "    - Solve the Markov decision process using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies\n",
    "    - Consider only discrete action methods, such as zero, low, medium, high, for the charging power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try mit Hilfe von diesem Tutorial:\n",
    "# https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # a range of 0 kW to 22 kW\n",
    "        #self.action_space = Box(low=0, high=22)\n",
    "        #a range from zero, low, medium to high\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # The vehicle's battery has a limited capacity that cannot be exceeded (69KWh)\n",
    "        #self.battery_space = Box(low=0, high=69)\n",
    "        self.battery_space = Box(low=np.array([0]), high = np.array([69]))\n",
    "\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #print(\"-- New Step --\")\n",
    "        # Setting loading interval -1 /--> -15 minutes\n",
    "        self.time += 1\n",
    "\n",
    "\n",
    "        # Seting new battery state\n",
    "        #zero\n",
    "        load = 0\n",
    "        if action == 2:\n",
    "            #low\n",
    "            load += 7\n",
    "        if action == 3:\n",
    "            #medium\n",
    "            load += 14\n",
    "        if action == 4:\n",
    "            #high\n",
    "            load += 22\n",
    "        self.battery_state += load\n",
    "\n",
    "\n",
    "        # Calculating Negative Reward from Energy Costs\n",
    "        kw_price = 1 #random.randint(0,3)\n",
    "        reward = kw_price * load * (-1)\n",
    "\n",
    "\n",
    "        #Checking if 2 Hours are done\n",
    "        #Giving panalty if car ran out of battery\n",
    "        if self.time >= 7:\n",
    "            #The taxi needs enough energy to complete its working day,\n",
    "            # which is a random value following a normal distribution\n",
    "            # (e.g., ðœ‡= 30 kWh, ðœŽ = 5 kWh)\n",
    "            kwh_needed = np.random.normal(loc=30, scale=5)\n",
    "            print(\"kwh needed:\" + str(kwh_needed))\n",
    "            print(\"battery state:\" + str(self.battery_state))\n",
    "            # The agentâ€™s goal is to avoid running out of energy (with a very high penalty) \n",
    "            if kwh_needed > self.battery_state:\n",
    "                reward -= 100\n",
    "            #print(\"Day done!\")\n",
    "            day_done = True\n",
    "        else:\n",
    "            day_done = False\n",
    "\n",
    "        #print(\"Battery State: \" + str(self.battery_state))\n",
    "        #print(\"Reward: \" + str(reward))\n",
    "        # Returning the step information\n",
    "        return self.battery_state, reward, day_done\n",
    "    \n",
    "    #def render(self):\n",
    "        # blabala\n",
    "    \n",
    "    def reset(self):\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "        #return self.battery_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ Day 1 ___\n",
      "kwh needed:29.42905358545278\n",
      "battery state:65\n",
      "Episode:1 Score:-28\n",
      "__ Day 2 ___\n",
      "kwh needed:29.99224850232707\n",
      "battery state:76\n",
      "Episode:2 Score:-42\n",
      "__ Day 3 ___\n",
      "kwh needed:37.11428092348993\n",
      "battery state:27\n",
      "Episode:3 Score:-107\n",
      "__ Day 4 ___\n",
      "kwh needed:31.505417424263786\n",
      "battery state:54\n",
      "Episode:4 Score:-28\n",
      "__ Day 5 ___\n",
      "kwh needed:26.08148921550992\n",
      "battery state:81\n",
      "Episode:5 Score:-56\n",
      "__ Day 6 ___\n",
      "kwh needed:21.20943629503527\n",
      "battery state:59\n",
      "Episode:6 Score:-35\n",
      "__ Day 7 ___\n",
      "kwh needed:27.363575893974534\n",
      "battery state:73\n",
      "Episode:7 Score:-42\n"
     ]
    }
   ],
   "source": [
    "episodes = 7 #7 days\n",
    "for episode in range(1, episodes+1):\n",
    "    print(\"__ Day \" + str(episode) + \" ___\")\n",
    "    #battery_state = \n",
    "    env.reset()\n",
    "    day_done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not day_done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, day_done = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = env.battery_space.shape\n",
    "#states = int(69)\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(69, activation='relu', input_shape=(69,)))\n",
    "    model.add(Dense(36, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m build_model(states, actions)\n",
      "Cell \u001b[1;32mIn[143], line 3\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(states, actions)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(states, actions):\n\u001b[0;32m      2\u001b[0m     model \u001b[39m=\u001b[39m Sequential()    \n\u001b[1;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39;49madd(Dense(\u001b[39m69\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m69\u001b[39;49m,)))\n\u001b[0;32m      4\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(\u001b[39m36\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(actions, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\NickPC\\.virtualenvs\\AAA_2023-67gSWIjI\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:285\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    284\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m VariableV1:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v1_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    286\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
