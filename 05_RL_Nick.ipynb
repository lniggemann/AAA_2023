{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to do some Reinforcement Learning -.-"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Smart Charging Using Reinforcement Learning:\n",
    "**Original Exercise:** <br>\n",
    "Consider an electric taxi driver who can charge her vehicle at home. To simplify the problem, we assume that the vehicle always arrives at home at 2 p.m. and leaves the garage at 4 p.m. each day. We want to design an intelligent charging system (an automated agent). Therefore, instead of a flat charging rate, the charging agent adjusts the charging power every 15 minutes, which is bounded between 0 kW and the highest rate (e.g., 22 kW). Also, the vehicle's battery has a capacity that cannot be exceeded. After leaving the garage, the taxi needs enough energy to complete its working day. The energy demand is a stochastic value following a normal distribution (you should choose the parameters, e.g., 𝜇= 30 kWh, 𝜎 = 5 kWh) and must be generated exactly when the driver wants to leave. The agent’s goal is to avoid running out of energy (you should consider a very high penalty for running out of energy) and to minimize the recharging cost. The recharging cost follows an exponential function of the power (i.e., ![image.png](attachment:image.png)), where 𝛼𝑡 is the time coefficient and p is the charging rate.\n",
    "\n",
    "The task is to create the environment (a very simple discrete event simulation) that receives the agent's decisions and returns the reward. In addition, you must define a Markov decision process, including states, actions, and reward function, and solve it using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies. To allow the use of discrete action methods, you can consider only limited charging options such as zero, low, medium, high.\n",
    "\n",
    "\n",
    "**In Bulletpoints:**\n",
    "- Problem description:\n",
    "    - An electric taxi driver can charge her vehicle at home between 2 p.m. and 4 p.m. each day\n",
    "    - The charging agent adjusts the charging power every 15 minutes within a range of 0 kW to 22 kW\n",
    "    - The vehicle's battery has a limited capacity that cannot be exceeded\n",
    "    - The taxi needs enough energy to complete its working day, which is a random value following a normal distribution (e.g., 𝜇= 30 kWh, 𝜎 = 5 kWh)\n",
    "    - The agent’s goal is to avoid running out of energy (with a very high penalty) and to minimize the recharging cost, which is an exponential function of the power (i.e., ![image.png](attachment:image.png)), where 𝛼𝑡 is the time coefficient and p is the charging rate\n",
    "- Task description:\n",
    "    - Create the environment that simulates the charging process and the energy demand, and returns the reward to the agent based on its actions\n",
    "    - Define a Markov decision process, including states, actions, and reward function, that models the problem\n",
    "    - Solve the Markov decision process using a reinforcement learning algorithm (e.g., deep q-network) to find optimal charging policies\n",
    "    - Consider only discrete action methods, such as zero, low, medium, high, for the charging power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try mit Hilfe von diesem Tutorial:\n",
    "# https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # a range of 0 kW to 22 kW\n",
    "        #self.action_space = Box(low=0, high=22)\n",
    "        #a range from zero, low, medium to high\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # The vehicle's battery has a limited capacity that cannot be exceeded (69KWh)\n",
    "        self.battery_space = Box(low=0, high=69)\n",
    "\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #print(\"-- New Step --\")\n",
    "        # Setting loading interval -1 /--> -15 minutes\n",
    "        self.time += 1\n",
    "\n",
    "        # Seting new battery state\n",
    "        if action == 2:\n",
    "            #low\n",
    "            self.battery_state += 7\n",
    "        if action == 3:\n",
    "            #medium\n",
    "            self.battery_state += 14\n",
    "        if action == 4:\n",
    "            #high\n",
    "            self.battery_state += 22\n",
    "\n",
    "\n",
    "\n",
    "        # Calculating Negative Reward from Energy Costs\n",
    "        kw_price = 1 #random.randint(0,3)\n",
    "        reward = kw_price * action * (-1)\n",
    "\n",
    "\n",
    "        #Checking if 2 Hours are done\n",
    "        #Giving panalty if car ran out of battery\n",
    "        if self.time >= 7:\n",
    "            #The taxi needs enough energy to complete its working day,\n",
    "            # which is a random value following a normal distribution\n",
    "            # (e.g., 𝜇= 30 kWh, 𝜎 = 5 kWh)\n",
    "            kwh_needed = np.random.normal(loc=30, scale=5)\n",
    "            print(\"kwh needed:\" + str(kwh_needed))\n",
    "            print(\"battery state:\" + str(self.battery_state))\n",
    "            # The agent’s goal is to avoid running out of energy (with a very high penalty) \n",
    "            if kwh_needed > self.battery_state:\n",
    "                reward -= 100\n",
    "            #print(\"Day done!\")\n",
    "            day_done = True\n",
    "        else:\n",
    "            day_done = False\n",
    "\n",
    "        #print(\"Battery State: \" + str(self.battery_state))\n",
    "        #print(\"Reward: \" + str(reward))\n",
    "        # Returning the step information\n",
    "        return self.battery_state, reward, day_done\n",
    "    \n",
    "    #def render(self):\n",
    "        # blabala\n",
    "    \n",
    "    def reset(self):\n",
    "        # [20,40] KWh loaded battery at initialization\n",
    "        self.battery_state = 30 + random.randint(-10,10)\n",
    "        # The charging agent adjusts the charging power every 15 minutes --> time is in [0,7] in 2 Hours\n",
    "        self.time = 0\n",
    "        #return self.battery_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ Day 1 ___\n",
      "kwh needed:34.4206140250764\n",
      "battery state:61\n",
      "Episode:1 Score:-9\n",
      "__ Day 2 ___\n",
      "kwh needed:36.12052365632374\n",
      "battery state:67\n",
      "Episode:2 Score:-13\n",
      "__ Day 3 ___\n",
      "kwh needed:34.394236563538534\n",
      "battery state:65\n",
      "Episode:3 Score:-13\n",
      "__ Day 4 ___\n",
      "kwh needed:35.045503228597525\n",
      "battery state:81\n",
      "Episode:4 Score:-13\n",
      "__ Day 5 ___\n",
      "kwh needed:34.50632972988435\n",
      "battery state:57\n",
      "Episode:5 Score:-8\n",
      "__ Day 6 ___\n",
      "kwh needed:33.63148472148491\n",
      "battery state:82\n",
      "Episode:6 Score:-13\n",
      "__ Day 7 ___\n",
      "kwh needed:28.12996354388714\n",
      "battery state:53\n",
      "Episode:7 Score:-4\n"
     ]
    }
   ],
   "source": [
    "episodes = 7 #7 days\n",
    "for episode in range(1, episodes+1):\n",
    "    print(\"__ Day \" + str(episode) + \" ___\")\n",
    "    #battery_state = \n",
    "    env.reset()\n",
    "    day_done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not day_done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, day_done = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.battery_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.battery_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAA_2023-67gSWIjI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
